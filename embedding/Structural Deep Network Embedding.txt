
Structural Deep Network Embedding

ABSTRACT
Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.
Keywords
Network Embedding, Deep Learning, Network Analysis
1. INTRODUCTION
Nowadays, networks are ubiquitous and many real-world applications need to mine the information within these networks. For example, recommendation system in Twitter aims to mine the preferred tweets for users from the social network. Online advertise-
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
KDD ’16, August 13-17, 2016, San Francisco, CA, USA
⃝c 2016ACM.ISBN978-1-4503-4232-2/16/08...$15.00 DOI: http://dx.doi.org/10.1145/2939672.2939753

ment targeting often needs to cluster the users into communities in the social network. Therefore, mining the information in the network is very important. One of the fundamental problems is how to learn useful network representations [5]. An effective way is to embed networks into a low-dimensional space, i.e. learn vector representations for each vertex, with the goal of reconstructing the network in the learned embedding space. As a result, mining information in networks, such as information retrieval [34], classification [15], and clustering [20], can be directly conducted in the low-dimensional space.
Learning network representations faces the following great challenges: (1) High non-linearity: As [19] stated, the underlying structure of the network is highly non-linear. Therefore, how to design a model to capture the highly non-linear structure is rather difficult. (2) Structure-preserving: To support applications analyzing networks, network embedding is required to preserve the network structure. However, the underlying structure of the network is very complex [24]. The similarity of vertexes is dependent on both the local and global network structure. Therefore, how to simultaneously preserve the local and global structure is a tough problem. (3) Sparsity: Many real-world networks are often so sparse that only utilizing the very limited observed links is not enough to reach a satisfactory performance [21].
In the past decades, many network embedding methods have been proposed, which adopted shallow models, such as IsoMAP [29], Laplacian Eigenmaps (LE) [1] and Line [26]. However, due to the limited representation ability of shallow models [2], it is difficult for them to capture the highly nonlinear network structure [30]. Although some methods adopt kernel techniques [32], as [36] stated, kernel methods are also shallow models and cannot capture the highly non-linear structure well.
In order to capture the highly non-linear structure well, in this paper we propose a new deep model to learn vertex representations for networks. This is motivated by the recent success of deep learning, which has been demonstrated to have a powerful representation ability to learn complex structures of the data [2] and has achieved substantial success in dealing with images [15], text [25] and audio [10] data. In particular, in our proposed model we design a multilayer architecture which consists of multiple non-linear functions. The composition of multiple layers of non-linear functions can map the data into a highly non-linear latent space, thereby being able to capture the highly non-linear network structure.
In order to address the structure-preserving and sparsity problems in the deep model, we further propose to exploit the first-order and second-order proximity [26] jointly into the learning process. The first-order proximity is the local pairwise similarity only between the vertexes linked by edges, which characterizes the local network structure. However, due to the sparsity of the network,

 25 20 15 10
        First−order Proximity Second−order Proximity
          arxiv−GrQc blogcatalog Flickr Youtube
dataset
Figure 1: The number of pairs of vertexes which have firstorder and second-order proximity in different datasets.
many legitimate links are missing. As a result, the first-order proximity is not sufficient to represent the network structure. Therefore, we further propose the second-order proximity, which indicates the similarity of the vertexes’ neighborhood structures, to capture the global network structure. With the first-order and second-order proximity, we can well characterize the local and global network structure, respectively. To preserve both the local and global network structure in the deep model, we propose a semi-supervised architecture, in which the unsupervised component reconstructs the second-order proximity to preserve the global network structure while the supervised component exploits the first-order proximity as the supervised information to preserve the local network structure. As a result, the learned representations can well preserve both the local and global network structure. In addition, as shown in Figure 1, the number of pairs of vertexes which have secondorder proximity is much huger than those have first-order proximity. Therefore, the import of second-order proximity is able to provide much more information in term of characterizing the network structure. As a result, our method is robust to sparse networks.
Empirically, we conduct the experiments on five real-world networked datasets and four real-world applications. The results show that compared with baselines, the representations generated by our method can reconstruct the original networks significantly better and achieve substantial gains on various tasks and various networks, including very sparse networks. It demonstrates that our representations learned in the highly non-linear space can preserve the network structure well and are robust to sparse networks.
In summary, the contributions of this paper are listed as follows:
• We propose a Structural Deep Network Embedding method, namely SDNE, to perform network embedding. The method is able to map the data to a highly non-linear latent space to preserve the network structure and is robust to sparse networks. To the best of our knowledge, we are among the first to use deep learning to learn network representations.
• We propose a new deep model with a semi-supervised architecture, which simultaneously optimizes the first-order and second-order proximity. As a result, the learned representations preserve the local and global network structure and are robust to sparse networks.
• The proposed method is extensively evaluated on five real datasets and four application scenarios. The results demonstrate the superior usefulness of the method in multi-label classification, reconstruction, link prediction and visualization. Specifically, our method can achieve more significant improvements (20%) over baselines when labelled data is scarce. In some cases we only need 60% less training samples but still achieve better performance.
2. RELATED WORK
2.1 Deep Neural Network
Representation learning has long been an important problem of machine learning and many works aim at learning representations for samples [3, 35]. Recent advances in deep neural networks have witnessed that they have powerful representations abilities [12] and can generate very useful representations for many types of data. For example, [15] proposed a seven-layer convolutional neural network to generate image representations for classification. [33] proposed a multimodal deep model to learn image-text unified representations to achieve cross-modality retrieval task.
However, to the best of our knowledge, there have been few deep learning works handling networks, especially learning network representations. In [9], Restricted Boltzmann Machines were adopted to do collaborative filtering. [30] adopted deep autoencoder to do graph clustering. [5] proposed a heterogeneous deep model to do heterogeneous data embedding. We differ from these works in two aspects. Firstly, the goals are different. Our work focuses on learning low-dimensional structure-preserved network representations which can be utilized among tasks. Secondly, we consider both the first-order and second-order proximity between vertexes to preserve the local and global network structure. But they only focus on one-order information.
2.2 Network Embedding
Our work solves the problem of network embedding, which aims to learn representations for networks. Some earlier works like Local Linear Embedding (LLE) [22], IsoMAP [29] first constructed the affinity graph based on the feature vectors and then solved the leading eigenvectors as the network representations. More recently, [26] designed two loss functions attempting to capture the local and global network structure respectively. Furthermore, [4] extended the work to utilize high-order information. Despite the success of these network embedding approaches, they all adopt shallow models. As we have explained earlier, it is difficult for shallow models to effectively capture the highly non-linear structure in the underlying network. In addition, although some of them attempt to use first-order and high-order proximity to preserve the local and global network structure, they learn the representations for them separately and simply concatenate the representations. Obviously, it is sub-optimal than simultaneously modeling them in a unified architecture to capture both the local and global network structure.
DeepWalk [21] combined random walk and skip-gram to learn network representations. Although empirically effective, it lacks a clear objective function to articulate how to preserve the network structure. It is prone to preserving only the second-order proximity. However, our method designs an explicit objective function, which aims at simultaneously preserving the local and global structure by preserving both the first-order and second-order proximity.
3.
In this section, we first define the problem. Then we introduce the proposed semi-supervised deep model of SDNE. At last we present some discussions and analysis on the model.
3.1 Problem Definition
We first give the definition of a Graph.
DEFINITION 1. (Graph) A graph is denoted as G = (V,E), whereV ={v1,...,vn}representsnvertexesandE={ei,j}ni,j=1 represents the edges. Each edge ei,j is associated with a weight
STRUCTURAL DEEP NETWORK EMBED-
DING
log(number)

 Unsupervised Component Unsupervised Component (Local structure preserved cost) (Local structure preserved cost)
